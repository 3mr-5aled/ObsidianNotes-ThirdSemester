# ðŸ“Š CIS240 - Statistical Analysis Lecture 03  
**By Dr. Ghada Hamed & Dr. Fatma Naguib**

---

## **Agenda**
- Descriptive Statistics  
- Measures of Dispersion (Variance & Standard Deviation)  
- Z-Score  

---

## **1ï¸âƒ£ Measures of Dispersion (Variability)**

### **A. Variance**
- Measures average **squared distance** of observations from the mean.  
- Formula (Ungrouped):  
  $$
  s^2 = \frac{\sum (x - \bar{x})^2}{n - 1}
  $$
- Reason for squaring: deviations above and below mean cancel out; squaring removes negatives.  
- For grouped data:  
  $$
  s^2 = \frac{\sum f x^2 - (\sum f x)^2/n}{n - 1}
  $$

### **B. Standard Deviation (SD)**
- Square root of variance; gives **average linear distance** from mean.  
- Formula:  
  $$
  s = \sqrt{\frac{\sum (x - \bar{x})^2}{n - 1}}
  $$
- Sensitive to outliers.  
- Units are same as data, unlike variance.  

**Interpretation:**  
Higher SD â†’ more spread in data.  
Lower SD â†’ data clustered around mean.  

---

## **2ï¸âƒ£ Z-Score (Standard Score)**

### **Definition**
- Indicates **how many standard deviations** an observation is from the mean.  
- Formula:  
  $$
  Z = \frac{x - \bar{x}}{s}
  $$
- Standardizes data for comparison across different scales.

### **Interpretation**
- $Z = 0$ â†’ at the mean  
- $Z > 0$ â†’ above the mean  
- $Z < 0$ â†’ below the mean  

**Applications:**  
- Detecting exceptional or common observations.  
- Comparing across distributions with different means and SDs.

---

## **3ï¸âƒ£ The Empirical Rule (Normal Distribution)**
Applicable for **bell-shaped** distributions:
- **68 %** of data â†’ within Â± 1 SD of mean  
- **95 %** â†’ within Â± 2 SD  
- **99.7 %** â†’ within Â± 3 SD  

**Example:**  
For IQ $(\mu = 100, \sigma = 10)$:  
- 90â€“110 â†’ 68 %  
- 80â€“120 â†’ 95 %  
- 70â€“130 â†’ 99.7 %  

---

## **4ï¸âƒ£ Chebyshevâ€™s Theorem (Any Distribution)**
- Works for **any shape** of data, not just normal.  
- At least $(1 - 1/k^2)$ of data lie within k SDs of mean.  
  - For $k = 2$ â†’ â‰¥ 75 %  
  - For $k = 3$ â†’ â‰¥ 89 %

---

## **5ï¸âƒ£ Worked Examples**

### **Example 1**
- Dataset: school grades.  
- Found outlier = 4.1 (below $Q_1 âˆ’ 1.5 IQR$).  
- Mean = 6.86, SD = 1.27.  
- Z-score used to confirm exceptional value.

### **Example 2**
- Heights (100 men): mean = 69.92 in, SD = 1.7 in.  
- 68 % within Â± 1 SD, 95 % within Â± 2, 100 % within Â± 3 SD.  

### **Example 3**
- Male height $(\mu = 69.6, \sigma = 1.4)$:  
  - 68.2â€“71.0 â†’ 68 %  
  - 66.8â€“72.4 â†’ 95 %  
  - 65.4â€“73.8 â†’ ~100 %
### **Example 4 (IQ Scores)**
- 110 â†’ 84th percentile.  
- 120 â†’ 97.5th.  
- 130 â†’ 99.85th.  
### **Example 5 (Chebyshevâ€™s)**
- $n = 50$, mean = 28, SD = 3  
- Interval (22, 34) = Â± 2 SD  
- At least 75 % (â‰ˆ 38 values) within interval.  
- At most 12 outside.  

---

## **6ï¸âƒ£ Key Takeaways**
- **Variance:** squared measure of spread.  
- **Standard Deviation:** practical measure (same unit as data).  
- **Z-Score:** standardizes scores for comparison.  
- **Empirical Rule:** for normal data.  
- **Chebyshevâ€™s Rule:** for any data.  
- Always check for **outliers** â€” they distort variability.  


#### Notes
here's when to use IQR, Z-score, Chebyshev's Theorem, or the Empirical Rule for identifying outliers:

1. **IQR (Interquartile Range)**
    
    - **When to use**: The note mentions an outlier being found "belowÂ " in Example 1. This method is particularly useful for detecting outliers, especially when the data isÂ **skewed**Â or not normally distributed, as it is less sensitive to extreme values than methods relying on the mean and standard deviation.
    - **How it works for outliers**: Values that fall belowÂ Â or aboveÂ Â are typically considered outliers.
2. **Z-Score (Standard Score)**
    
    - **When to use**: The note states Z-scores are used for "Detecting exceptional or common observations" and "Standardizes data for comparison across different scales." You would use Z-scores when you want to understand how far an individual data point is from the mean in terms of standard deviations.
    - **How it works for outliers**: Observations with a Z-score that has a large absolute value (e.g., typicallyÂ Â orÂ ) are considered exceptional or potential outliers, as they are many standard deviations away from the mean.
3. **Empirical Rule**
    
    - **When to use**: This rule is "Applicable forÂ **bell-shaped**Â distributions" (i.e., approximately normal distributions). You would use it when you have data that you know or suspect follows a normal distribution.
    - **How it works for outliers**:
        - Approximately 95% of data falls withinÂ Â standard deviations of the mean.
        - Approximately 99.7% of data falls withinÂ Â standard deviations of the mean.
        - Therefore, any data point falling outsideÂ Â standard deviations (especially outsideÂ Â standard deviations) is a strong candidate for an outlier in a normal distribution.
4. **Chebyshev's Theorem**
    
    - **When to use**: This theorem "Works forÂ **any shape**Â of data, not just normal." You would use Chebyshev's Theorem when you need to identify potential outliers in a dataset whose distribution shape is unknown or is known to be non-normal.
    - **How it works for outliers**: It provides aÂ _minimum_Â percentage of data that must lie within a certain number of standard deviations from the mean. For example, at least 75% of data lies withinÂ Â standard deviations, and at least 89% withinÂ Â standard deviations. If a data point falls far outside these bounds, it is an outlier. While less precise than the Empirical Rule for normal data, it offers a robust, conservative estimate for any distribution.

In summary:

- UseÂ **IQR**Â for skewed data or when you want a robust method less affected by extreme values.
- UseÂ **Z-score**Â to quantify how "unusual" a specific data point is relative to the mean and standard deviation, especially for roughly symmetric data.
- Use theÂ **Empirical Rule**Â for data that is known to be or closely approximates a bell-shaped (normal) distribution.
- UseÂ **Chebyshev's Theorem**Â forÂ _any_Â data distribution when you need a general, conservative estimate of how much data falls within a certain range, and thus to identify values that are exceptionally far from the mean.

Ø¹Ø´Ø§Ù† Ø§Ù„data Ø¹Ù†Ø¯Ù†Ø§ Ù…Ø´ Ù‡ØªØ¹Ù…Ù„ normal distribution Ø¹Ø´Ø§Ù† Ù†Ø·Ø¨Ù‚ Ø§Ù„emprical rule Ùˆ Ø§Ù„outliers Ø¨Ù†Ø¬ÙŠØ¨Ù‡Ù… Ø¨Ø§Ù„IQR Ø²ÙŠ Ù…Ø§ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø³Ù„Ø§ÙŠØ¯ Ø¥Ù„ÙŠ Ù‚Ø¨Ù„Ù‡Ø§